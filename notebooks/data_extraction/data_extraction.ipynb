{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import yaml\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# analysis libraries\n",
    "import osmnx as ox\n",
    "import momepy as mm\n",
    "import random\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "# visualization libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# directories\n",
    "datasets_dir = \"datasets/\"\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "req_path = datasets_dir + \"osmdata_requests.yaml\"\n",
    "open_path = datasets_dir + \"opendata_requests.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# check if directory already exist, otherwise create them\n",
    "if os.path.exists(datasets_dir):\n",
    "    pass\n",
    "else:\n",
    "    datasets_dir = \"datasets/\"\n",
    "    os.makedirs(datasets_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect polygons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_polygons = gpd.read_file(datasets_dir + \"bologna_polygons.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_polygons= raw_polygons.set_crs(epsg=4326, allow_override=True)\n",
    "data_polygons = raw_polygons.to_crs(epsg=32632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_polygons.plot(figsize=(10, 10), edgecolor=\"w\").set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute area\n",
    "data_polygons['area'] = data_polygons['geometry'].area / 1e6 # better readability (squared kilometers)\n",
    "\n",
    "# show df\n",
    "data_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect data related to openstreetmap\n",
    "optional: to run only when raw data do not exist\n",
    "NB: we retrieve data using the EPSG:32632 coordinates, which refers to WGS 84 / UTM zone 32N (uses meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENSTREETMAP DATA\n",
    "\n",
    "# read the request file\n",
    "with open(req_path, 'r') as f:\n",
    "    data_requests = yaml.safe_load(f)\n",
    "\n",
    "# initialize the empty dataframe\n",
    "all_data = pd.DataFrame(columns=[\"id\", \"lat\", \"lon\", \"class\"])\n",
    "\n",
    "# for each urban function retrieve the corresponding data points from openstreetmap\n",
    "for el in data_requests:\n",
    "    response = requests.get(overpass_url,\n",
    "                            params={'data': data_requests[el]})\n",
    "    #print(response)\n",
    "    raw_data = response.json()\n",
    "    elements = raw_data.get('elements', [])\n",
    "\n",
    "    rows = []\n",
    "    for item in elements:\n",
    "        # for some urban functions we retrieve the points directly as nodes\n",
    "        if item.get('type') == 'node':\n",
    "            lat = item.get('lat')\n",
    "            lon = item.get('lon')\n",
    "            if lat is None or lon is None:\n",
    "                continue\n",
    "\n",
    "        # for other instead we retrieve the area and then identify the coordinates of the centroid\n",
    "        elif 'center' in item:\n",
    "            center = item['center']\n",
    "            lat = center.get('lat')\n",
    "            lon = center.get('lon')\n",
    "            if lat is None or lon is None:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        rows.append({\n",
    "            'id': item.get('id'),\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'class': el\n",
    "        })\n",
    "\n",
    "    gdf_tmp = pd.DataFrame(rows)\n",
    "    all_data = pd.concat([all_data, gdf_tmp], ignore_index=True)\n",
    "\n",
    "    print(f\"OSM data retrieved correctly for {el} urban function: {gdf_tmp.shape[0]} nodes/areas were collected\")\n",
    "    time.sleep(3)  # to avoid blocking from Overpass API\n",
    "\n",
    "    print(f\"current total count of PoIs: {all_data.shape[0]}\")\n",
    "\n",
    "final_gdf_osm = gpd.GeoDataFrame(\n",
    "    all_data,\n",
    "    geometry=gpd.points_from_xy(all_data.lon, all_data.lat),\n",
    "    crs=\"EPSG:32632\"\n",
    ")\n",
    "\n",
    "print(f\"For the municipality of Bologna, {final_gdf_osm.shape[0]} Points of Interest related to OpenStreetMap were collected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect data from Bologna OpenData Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN THE DATASET\n",
    "with open(open_path, \"r\") as f:\n",
    "    opendata_requests = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE ALL DATA from Bologna Open Data Portal\n",
    "def retrieve_open_data(base_url):\n",
    "    \n",
    "    # here we cannot retrieve all data together so we create multiple batches\n",
    "    offset = 0\n",
    "    limit = 100\n",
    "    all_records = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        paged_url = f\"{base_url}&limit={limit}&offset={offset}\" # change everytime the batches\n",
    "        \n",
    "        response = requests.get(paged_url) # extract the raw data\n",
    "        json_resp = response.json() \n",
    "        records = json_resp.get(\"results\", []) # extract the data we actually want\n",
    "\n",
    "        if not records:\n",
    "            break\n",
    "\n",
    "        all_records.extend(records)\n",
    "\n",
    "        if len(records) < limit: # reached the limit of data available\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "    \n",
    "    return pd.DataFrame(all_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO ASSIGN A RANDOM UNIQUE ID\n",
    "def assign_unique_4digit_ids(df, used_ids=set()):\n",
    "    n = len(df)\n",
    "    possible_ids = set(range(1000, 10000)) - used_ids\n",
    "\n",
    "    if len(possible_ids) < n:\n",
    "        raise ValueError(\"not enough unique 4-digit IDs left to assign\")\n",
    "\n",
    "    unique_ids = random.sample(list(possible_ids), n)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['id'] = unique_ids\n",
    "\n",
    "    return df, used_ids.union(unique_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_ids = set() # the initial pool of ids is empty \n",
    "\n",
    "# we extract two different datasets from the Bologna Open Data Portal\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GREEN URBAN FUNCTION EXTRACTION\n",
    "green_df = retrieve_open_data(opendata_requests[\"green\"][0]) # retrieve entire dataframe\n",
    "parchi_df = green_df[green_df['tipo'].isin(['PARCO', 'GIARDINI'])] # extract only public gardens and parks\n",
    "\n",
    "parchi_df, used_ids = assign_unique_4digit_ids(parchi_df, used_ids) # assign unique ids\n",
    "parchi_df['class'] = 'green'\n",
    "parchi_df['lat'] = parchi_df['geo_point_2d'].apply(lambda x: x['lat'])\n",
    "parchi_df['lon'] = parchi_df['geo_point_2d'].apply(lambda x: x['lon'])\n",
    "\n",
    "parchi_df = parchi_df[['id', 'lat', 'lon', 'class']]\n",
    "parchi_gdf = gpd.GeoDataFrame(parchi_df,\n",
    "                             geometry=gpd.points_from_xy(parchi_df.lon, parchi_df.lat),\n",
    "                             crs=\"EPSG:32632\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all the data points retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([final_gdf_osm, parchi_gdf], ignore_index=True) # combine all the datasets\n",
    "combined_df = combined_df.set_crs('EPSG:4326', allow_override=True) # change crs\n",
    "combined_df # check the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT BY URBAN FUNCTION\n",
    "fig = px.scatter_map(combined_df,\n",
    "                        lat='lat',\n",
    "                        lon='lon',\n",
    "                        color='class',\n",
    "                        hover_name='id',\n",
    "                        zoom=12,\n",
    "                        map_style='open-street-map',\n",
    "                        title=f'Distribution of PoIs retrieved based on urban function'\n",
    "                     )\n",
    "fig.show() #renderer='notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning\n",
    "Here we want to check (and possibly remove) data points that could be doubled or replicated. To do so, we work by class, urban function, and try to detect where more than three points are either inside a small buffer (50 m) or aligned on the same street for more than 50 meters. \n",
    "This could imply that such data points refer to the same original structure but have multiple facilities concentrated in the same area.\n",
    "The only exception we consider is schools, where it is common that inside the same building are present different school grades (elementary, middle...) -> here, even the function is the same, the type of school is different and we want to keep this type of information inside our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_points = []\n",
    "group_id = 0\n",
    "\n",
    "# remove education from dataset\n",
    "combined_df_noedu = combined_df[combined_df['class'] != 'education'].copy()\n",
    "combined_df_noedu = combined_df_noedu.to_crs(epsg=32632)\n",
    "\n",
    "# now for each class\n",
    "for cls, group in combined_df_noedu.groupby('class'):\n",
    "    \n",
    "    # and for each data point\n",
    "    for idx, row in group.iterrows():\n",
    "        \n",
    "        # create a buffer of 30 meters\n",
    "        buffer = row.geometry.buffer(50)\n",
    "        \n",
    "        # count how many elements are inside this buffer\n",
    "        nearby = group[group.geometry.within(buffer)]\n",
    "        \n",
    "        # if more than 3 elements\n",
    "        if len(nearby) >= 3:\n",
    "            temp = nearby.copy()\n",
    "            temp[\"group_id\"] = group_id # assign a common identification number\n",
    "            grouped_points.append(temp) # add it to our dataframe\n",
    "            group_id += 1 # with a group id\n",
    "\n",
    "# now we merge the close data points such that it becomes a single data point\n",
    "flagged_points = pd.concat(grouped_points).drop_duplicates(subset=['id'])\n",
    "\n",
    "centroids = (\n",
    "    flagged_points\n",
    "    .dissolve(by='group_id', as_index=False)\n",
    "    .copy()\n",
    ")\n",
    "centroids['geometry'] = centroids.geometry.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual check of what we're doing\n",
    "flagged_points = flagged_points.to_crs(epsg=4326)\n",
    "\n",
    "# PLOT BY URBAN FUNCTION\n",
    "fig = px.scatter_map(flagged_points,\n",
    "                        lat='lat',\n",
    "                        lon='lon',\n",
    "                        color='class',\n",
    "                        hover_name='id',\n",
    "                        zoom=12,\n",
    "                        map_style='open-street-map',\n",
    "                        title=f'Distribution of too close data points (before cleaning: {flagged_points.shape[0]})'\n",
    "                     )\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see:\n",
    "- administration has three data points grouped very close. Two of them, along Piazza Galileo Galilei have also the same name (\"Questura\"), while the third one along Via Quattro Novembre is the Municipality Palace. Can be maintained separated.\n",
    "- healthcare has three facilities (appeared to be pharmacies) very close referring to different commercial activites, thus they should be separaterd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nodes - polygons alignment\n",
    "For further analysis we're going to consider the polygons separately therefore it is important to have a correspondence between nodes and polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial settings\n",
    "data_proj = combined_df.to_crs(epsg=32632)\n",
    "polygons_proj = data_polygons.to_crs(epsg=32632)\n",
    "polygons_proj.id = polygons_proj.id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPATIAL JOIN: we want to have a column in data dataset that identifies the grid polygon that contains each node\n",
    "new_data = gpd.sjoin(data_proj, polygons_proj, how=\"left\", predicate=\"within\")\n",
    "new_data = new_data.rename(columns={\n",
    "    \"id_right\": \"polygon_id\",\n",
    "    \"id_left\": \"node_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR OUTSIDE BOUNDARIES DATA\n",
    "nan_data = new_data[new_data.polygon_id.isna()]\n",
    "print(f\"data not in any polygon: {nan_data.shape[0]}\")\n",
    "\n",
    "new_data = new_data[~new_data.polygon_id.isna()]\n",
    "print(f\"data kept: {new_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data [['node_id','lat','lon','class','geometry','polygon_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data visualization\n",
    "Let's plot the data retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_gdf = data_polygons.to_crs(epsg=4326) # change to unprojected coordinates\n",
    "\n",
    "# scatter map for the data points retrieved\n",
    "scatter_points = go.Scattermap(\n",
    "    lat=new_data['lat'],\n",
    "    lon=new_data['lon'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='green'),\n",
    "    text=new_data['node_id'], \n",
    "    name='POIs'\n",
    ")\n",
    "\n",
    "# here we need to convert the polygons into plotly figures (still polygons!)\n",
    "polygons_traces = []\n",
    "\n",
    "for _, row in polygons_gdf.iterrows():\n",
    "    geom = row.geometry\n",
    "    name = row.get('polygon_id')  \n",
    "\n",
    "    coords = list(geom.exterior.coords)\n",
    "    lons, lats = zip(*coords)\n",
    "\n",
    "    polygons_traces.append(go.Scattermap(\n",
    "        lat=lats,\n",
    "        lon=lons,\n",
    "        mode='lines',\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(0, 100, 0, 0.2)',\n",
    "        line=dict(color='darkgreen'),\n",
    "        name=name\n",
    "        ))\n",
    "\n",
    "\n",
    "# create open-street-map layout\n",
    "layout = go.Layout(\n",
    "    mapbox=dict(\n",
    "        style=\"open-street-map\",\n",
    "        zoom=12,\n",
    "        center=dict(lat=combined_df['lat'].mean(), lon=combined_df['lon'].mean())\n",
    "    ),\n",
    "    title=\"PoIs distribtuion in the city of Bologna\"\n",
    ")\n",
    "\n",
    "# actual plot\n",
    "fig = go.Figure(data=[scatter_points] + polygons_traces, layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE COMPLETE DATASET\n",
    "\n",
    "geojson_path = os.path.join(datasets_dir, \"geopoints_data.geojson\")\n",
    "new_data.to_file(geojson_path, driver='GeoJSON')\n",
    "\n",
    "print(f\"dataset stored in directory: {geojson_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detailed information on dataset\n",
    "Here we want to gather information on the original polygon data and their buffered versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORIGINAL POLYGONS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA POINTS DISTRIBUTION BY ORIGINAL POLYGONS\n",
    "poi_count = new_data.groupby(\"polygon_id\").count()\n",
    "poi_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot purposes we change the crs system Web Mercator (EPSG:3857) -> to match basemap tiles\n",
    "data_polygons = data_polygons.to_crs(epsg=3857)\n",
    "data_polygons.id = data_polygons.id.astype(str)\n",
    " \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "data_polygons.plot(\n",
    "    ax=ax,\n",
    "    column='id',          \n",
    "    cmap='Set2',        \n",
    "    alpha=0.5,            \n",
    "    edgecolor='black',    \n",
    "    linewidth=0.5,\n",
    "    legend=True)\n",
    "\n",
    "\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)  \n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.title(\"Polygons identified in the municipality of Bologna\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# to store\n",
    "output_path = os.path.join(\"images\", \"bologna_polygons.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 M BUFFER POLYGONS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATIONS\n",
    "buffered_100 = []\n",
    "\n",
    "for el in data_polygons.id.unique():\n",
    "    \n",
    "    # here we compute the buffered polygon\n",
    "    geom_proj = polygons_proj.loc[int(el)].geometry.buffer(100) # add a 100 meters buffer to the polygon\n",
    "    \n",
    "    # and look for the data points contained in it\n",
    "    buffered_polygon = gpd.GeoDataFrame(geometry=[geom_proj], crs=\"EPSG:32632\")\n",
    "    subset_data_buffered = gpd.sjoin(new_data, buffered_polygon, predicate=\"within\")\n",
    "    \n",
    "    buffered_100.append({'id': el, 'geometry': geom_proj, 'POIs': subset_data_buffered.shape[0]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered100_polygons = gpd.GeoDataFrame(buffered_100, crs=\"EPSG:32632\")\n",
    "buffered100_polygons['area'] = buffered100_polygons.geometry.area /1e6\n",
    "buffered100_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot purposes we change the crs system Web Mercator (EPSG:3857) -> to match basemap tiles\n",
    "buffered50_polygons = buffered50_polygons.to_crs(epsg=3857)\n",
    "buffered50_polygons.id = buffered50_polygons.id.astype(str)\n",
    " \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "buffered50_polygons.plot(\n",
    "    ax=ax,\n",
    "    column='id',          \n",
    "    cmap='Set2',        \n",
    "    alpha=0.5,            \n",
    "    edgecolor='black',    \n",
    "    linewidth=0.5,\n",
    "    legend=True)\n",
    "\n",
    "\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)  \n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.title(\"Polygons identified in the municipality of Bologna - 50 m buffer\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# to store\n",
    "output_path = os.path.join(\"images\", \"bologna_polygons_50m.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200 M BUFFER POLYGONS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATIONS\n",
    "buffered_200 = []\n",
    "\n",
    "for el in data_polygons.id.unique():\n",
    "    \n",
    "    # here we compute the buffered polygon\n",
    "    geom_proj = polygons_proj.loc[int(el)].geometry.buffer(200) # add a 100 meters buffer to the polygon\n",
    "    \n",
    "    # and look for the data points contained in it\n",
    "    buffered_polygon = gpd.GeoDataFrame(geometry=[geom_proj], crs=\"EPSG:32632\")\n",
    "    subset_data_buffered = gpd.sjoin(new_data, buffered_polygon, predicate=\"within\")\n",
    "    \n",
    "    buffered_200.append({'id': el, 'geometry': geom_proj, 'POIs': subset_data_buffered.shape[0]})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered200_polygons = gpd.GeoDataFrame(buffered_200, crs=\"EPSG:32632\")\n",
    "buffered200_polygons['area'] = buffered200_polygons.geometry.area /1e6\n",
    "buffered200_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot purposes we change the crs system Web Mercator (EPSG:3857) -> to match basemap tiles\n",
    "buffered100_polygons = buffered100_polygons.to_crs(epsg=3857)\n",
    "buffered100_polygons.id = buffered100_polygons.id.astype(str)\n",
    " \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "buffered100_polygons.plot(\n",
    "    ax=ax,\n",
    "    column='id',          \n",
    "    cmap='Set2',        \n",
    "    alpha=0.5,            \n",
    "    edgecolor='black',    \n",
    "    linewidth=0.5,\n",
    "    legend=True)\n",
    "\n",
    "\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)  \n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.title(\"Polygons identified in the municipality of Bologna - 100 m buffer\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# to store\n",
    "output_path = os.path.join(\"images\", \"bologna_polygons_100m.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATIONS\n",
    "buffered_200 = []\n",
    "\n",
    "for el in data_polygons.id.unique():\n",
    "    \n",
    "    # here we compute the buffered polygon\n",
    "    geom_proj = polygons_proj.loc[int(el)].geometry.buffer(200) # add a 100 meters buffer to the polygon\n",
    "    \n",
    "    # and look for the data points contained in it\n",
    "    buffered_polygon = gpd.GeoDataFrame(geometry=[geom_proj], crs=\"EPSG:32632\")\n",
    "    subset_data_buffered = gpd.sjoin(new_data, buffered_polygon, predicate=\"within\")\n",
    "    \n",
    "    buffered_200.append({'id': el, 'geometry': geom_proj, 'POIs': subset_data_buffered.shape[0]})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered200_polygons = gpd.GeoDataFrame(buffered_200, crs=\"EPSG:32632\")\n",
    "buffered200_polygons['area'] = buffered200_polygons.geometry.area /1e6\n",
    "buffered200_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered200_polygons = buffered200_polygons.to_crs(epsg=3857)\n",
    "buffered200_polygons.id = buffered200_polygons.id.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPLETE PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# plot original polygons\n",
    "data_polygons.plot(\n",
    "    ax=ax,\n",
    "    column='id',\n",
    "    cmap='Set2',\n",
    "    alpha=0.5,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5,\n",
    "    legend=True,\n",
    ")\n",
    "\n",
    "# plot buffered polygons (transparent fill, red edges for distinction)\n",
    "buffered100_polygons.boundary.plot(\n",
    "    ax=ax,\n",
    "    color='red',\n",
    "    linewidth=1,\n",
    "    linestyle='--',\n",
    "    label='Buffered 100m'\n",
    ")\n",
    "\n",
    "# plot buffered polygons (transparent fill, red edges for distinction)\n",
    "buffered200_polygons.boundary.plot(\n",
    "    ax=ax,\n",
    "    color='blue',\n",
    "    linewidth=1,\n",
    "    linestyle='--',\n",
    "    label='Buffered 200m'\n",
    ")\n",
    "# add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.title(\"Polygons and Their 100m Buffers - Bologna\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# save if desired\n",
    "output_path = os.path.join(\"images\", \"bologna_polygons_with_buffers.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
